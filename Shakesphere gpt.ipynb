{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "bff913a6-4a83-4a55-af0f-74305d64161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my imports\n",
    "import pandas as pd\n",
    "#requests is a popular python libarary for making HTTP requests, such as downloading from URLs\n",
    "import requests\n",
    "import urllib.request\n",
    "import sentencepiece as spm\n",
    "import tiktoken \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7fc04c-6f69-421e-848c-28f5793a1929",
   "metadata": {},
   "source": [
    "## My Variables \n",
    "- vocab size=size of unique characters\n",
    "- characters=the sorted list of unique characters\n",
    "- train_data=data used to train the model\n",
    "- test_data=data used to test the model\n",
    "- context_length=# of characters the model references to make a prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a4fca669-42de-458f-b538-39d74b7dde36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyper-parameters \n",
    "characters=sorted(list(set(text)))\n",
    "size_vocab=len(characters)\n",
    "batch_size=35\n",
    "sequence_length=8\n",
    "num_epochs=5000\n",
    "interval_eval=500\n",
    "learning_rate=1e-2\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu' \n",
    "iters_eval=200\n",
    "dim_embd=32\n",
    "vocab_size=32\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67eb66-c9cc-43bf-8e13-fab12168886c",
   "metadata": {},
   "source": [
    "## General Notes\n",
    "- tensor: a gernalization of scalers, vectors, matrices to a higher dimensional space/multi-dimensional arrays/primary data structure used to store and manipulate date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d0c428b9-1341-41b3-9b86-f4b839a9280a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#got the url by open the raw version of the file in github \n",
    "url = \"https://raw.githubusercontent.com/yasminho/Shakespeare-gpt/main/input.txt?token=GHSAT0AAAAAACDEOEB7HCMDF43ROSZND5FAZF6U3OA\"\n",
    "try:\n",
    "    # Download the dataset using urllib\n",
    "    #downloads the contents from url and saves it as input.txt \n",
    "    urllib.request.urlretrieve(url, 'input.txt')\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "#if there is an exception, the code will enter the exception block \n",
    "except Exception as e:\n",
    "    print(f\"the dataset did not download: {e}\")\n",
    "\n",
    "#will enter the exception block and print the exception "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b9ec3cd5-5a00-4a8f-9f3e-ee839194eb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "#reading in the dataset \n",
    "#'r' means read \n",
    "#common encoding for text files \n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "\n",
    "print(len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "22392cf8-7fff-4184-aeee-1650fd323f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a34b3342-dd04-43fb-aded-15b9ea2ea41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "#list of the set of characters in the text \n",
    "characters=sorted(list(set(text)))\n",
    "#number of unique characters in the text \n",
    "size_vocab=len(characters)\n",
    "\n",
    "print(size_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50f75e-585f-4438-888c-7972772c3c5f",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8d8aa9d8-3ca4-43e2-b013-b5e4b24bb906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 47, 1, 37, 39, 57]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a mappings\n",
    "#dictionary mapping characters to corresponding integers using enumerate\n",
    "#enumerate returns an iterator that produces tuples containing index and the element from input sequence \n",
    "chars_int_mapping={char:int for int, char in enumerate(characters)}\n",
    "#dictionary mpping integers to corresponding characters \n",
    "int_chars_mapping={int: char for int, char in enumerate(characters)}\n",
    "#takes in a string and encodes it into a list of integers using our mapping/converts each character in string to corresponding integer value and returns list of integers\n",
    "encoder=lambda str:[chars_int_mapping[char] for char in str]\n",
    "#takes in a list of integers and decoes it to a string using our int_chars_mapping\n",
    "decoder=lambda int_list: ''.join([int_chars_mapping[i] for i in int_list])\n",
    "\n",
    "#when we use this encoder, we will get a list of numbers between 0-65 because that is the size of vocabulary \n",
    "#notice that we get a token for each character\n",
    "encoder(\"Hi Yas\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4001b4af-f89a-431d-b6e0-b4eacbee87eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17250]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizing with tiktoken \n",
    "tik_token_encoder=tiktoken.get_encoding('gpt2')\n",
    "#instead of 65 tokens (our original vocabulary size), it has 50257 tokens \n",
    "#when we use this encoder \n",
    "tik_token_encoder.n_vocab\n",
    "\n",
    "#when we use tik_token_encoder, we get a number that anywhere from 0-50257\n",
    "#notice that we get a token for each word \n",
    "tik_token_encoder.encode(\"Hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b4adaf-78ae-443a-bde7-475ccf2da97a",
   "metadata": {},
   "source": [
    "# splitting data\n",
    "- splitting into train and test set\n",
    "- 80% will train and 20% will test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a53b335c-3faf-4a8b-ae76-5bd30d3b2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look into nested-cross-validation\n",
    "encoded_data=torch.tensor(encoder(text), dtype=torch.long)\n",
    "train_length=int(len(encoded_data)*0.90)\n",
    "train_data=encoded_data[:train_length]\n",
    "test_data=encoded_data[train_length:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e19448-3257-43bd-9064-be66c6e841a4",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "90c93a07-c88d-4439-8b89-827ef5ebc808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([35, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54],\n",
      "        [57, 43, 60, 43, 52,  1, 63, 43],\n",
      "        [60, 43, 42,  8,  0, 25, 63,  1],\n",
      "        [56, 42,  5, 57,  1, 57, 39, 49],\n",
      "        [43, 57, 58, 63,  6,  1, 58, 46],\n",
      "        [43,  1, 51, 39, 63,  1, 40, 43],\n",
      "        [58, 46, 43,  1, 43, 39, 56, 57],\n",
      "        [39, 58, 47, 53, 52, 12,  1, 37],\n",
      "        [53, 56, 43,  1, 21,  1, 41, 39],\n",
      "        [50, 39, 52, 63,  1, 47, 58, 57],\n",
      "        [56, 53, 63,  1, 42, 47, 42,  1],\n",
      "        [39, 51,  1, 39, 44, 56, 39, 47],\n",
      "        [17, 24, 21, 38, 13, 14, 17, 32],\n",
      "        [ 1, 39, 52, 42,  1, 45, 43, 50],\n",
      "        [ 1, 58, 46, 39, 58,  1, 42, 53],\n",
      "        [ 1, 61, 53, 59, 50, 42,  1, 21],\n",
      "        [59, 57, 40, 39, 52, 42,  1, 40],\n",
      "        [52, 42,  8,  0,  0, 23, 21, 26],\n",
      "        [45, 53, 42, 57,  0, 23, 43, 43],\n",
      "        [52,  1, 61, 39, 57,  1, 51, 53],\n",
      "        [39, 49, 12,  1, 27,  1, 58, 56],\n",
      "        [53, 44,  1, 57, 54, 43, 43, 41],\n",
      "        [57, 53, 52, 57,  8,  0,  0, 25],\n",
      "        [ 1, 42, 43, 44, 43, 41, 58,  1],\n",
      "        [21,  1, 61, 39, 52, 42, 43, 56],\n",
      "        [43, 43, 51,  5, 42,  1, 40, 59],\n",
      "        [45, 50, 63,  1, 52, 53, 61, 12],\n",
      "        [52, 53, 58,  8,  0, 25, 63,  1],\n",
      "        [53, 58,  6,  1, 51, 63,  1, 50],\n",
      "        [52, 57,  8,  0, 21,  5, 50, 50],\n",
      "        [47, 58, 46,  1, 63, 53, 59,  6],\n",
      "        [ 1, 53, 40, 43, 63, 12,  0, 26]])\n",
      "prediction:\n",
      "torch.Size([35, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39],\n",
      "        [43, 60, 43, 52,  1, 63, 43, 39],\n",
      "        [43, 42,  8,  0, 25, 63,  1, 45],\n",
      "        [42,  5, 57,  1, 57, 39, 49, 43],\n",
      "        [57, 58, 63,  6,  1, 58, 46, 47],\n",
      "        [ 1, 51, 39, 63,  1, 40, 43,  1],\n",
      "        [46, 43,  1, 43, 39, 56, 57, 10],\n",
      "        [58, 47, 53, 52, 12,  1, 37, 53],\n",
      "        [56, 43,  1, 21,  1, 41, 39, 51],\n",
      "        [39, 52, 63,  1, 47, 58, 57, 43],\n",
      "        [53, 63,  1, 42, 47, 42,  1, 57],\n",
      "        [51,  1, 39, 44, 56, 39, 47, 42],\n",
      "        [24, 21, 38, 13, 14, 17, 32, 20],\n",
      "        [39, 52, 42,  1, 45, 43, 50, 42],\n",
      "        [58, 46, 39, 58,  1, 42, 53,  1],\n",
      "        [61, 53, 59, 50, 42,  1, 21,  1],\n",
      "        [57, 40, 39, 52, 42,  1, 40, 47],\n",
      "        [42,  8,  0,  0, 23, 21, 26, 19],\n",
      "        [53, 42, 57,  0, 23, 43, 43, 54],\n",
      "        [ 1, 61, 39, 57,  1, 51, 53, 56],\n",
      "        [49, 12,  1, 27,  1, 58, 56, 39],\n",
      "        [44,  1, 57, 54, 43, 43, 41, 46],\n",
      "        [53, 52, 57,  8,  0,  0, 25, 17],\n",
      "        [42, 43, 44, 43, 41, 58,  1, 53],\n",
      "        [ 1, 61, 39, 52, 42, 43, 56,  6],\n",
      "        [43, 51,  5, 42,  1, 40, 59, 56],\n",
      "        [50, 63,  1, 52, 53, 61, 12,  0],\n",
      "        [53, 58,  8,  0, 25, 63,  1, 61],\n",
      "        [58,  6,  1, 51, 63,  1, 50, 53],\n",
      "        [57,  8,  0, 21,  5, 50, 50,  1],\n",
      "        [58, 46,  1, 63, 53, 59,  6,  1],\n",
      "        [53, 40, 43, 63, 12,  0, 26, 39]])\n",
      "----\n",
      "when input is [24] the prediction: 43\n",
      "when input is [24, 43] the prediction: 58\n",
      "when input is [24, 43, 58] the prediction: 5\n",
      "when input is [24, 43, 58, 5] the prediction: 57\n",
      "when input is [24, 43, 58, 5, 57] the prediction: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the prediction: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the prediction: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the prediction: 39\n",
      "when input is [44] the prediction: 53\n",
      "when input is [44, 53] the prediction: 56\n",
      "when input is [44, 53, 56] the prediction: 1\n",
      "when input is [44, 53, 56, 1] the prediction: 58\n",
      "when input is [44, 53, 56, 1, 58] the prediction: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the prediction: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the prediction: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the prediction: 1\n",
      "when input is [52] the prediction: 58\n",
      "when input is [52, 58] the prediction: 1\n",
      "when input is [52, 58, 1] the prediction: 58\n",
      "when input is [52, 58, 1, 58] the prediction: 46\n",
      "when input is [52, 58, 1, 58, 46] the prediction: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the prediction: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the prediction: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the prediction: 46\n",
      "when input is [25] the prediction: 17\n",
      "when input is [25, 17] the prediction: 27\n",
      "when input is [25, 17, 27] the prediction: 10\n",
      "when input is [25, 17, 27, 10] the prediction: 0\n",
      "when input is [25, 17, 27, 10, 0] the prediction: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the prediction: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the prediction: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the prediction: 39\n",
      "when input is [57] the prediction: 43\n",
      "when input is [57, 43] the prediction: 60\n",
      "when input is [57, 43, 60] the prediction: 43\n",
      "when input is [57, 43, 60, 43] the prediction: 52\n",
      "when input is [57, 43, 60, 43, 52] the prediction: 1\n",
      "when input is [57, 43, 60, 43, 52, 1] the prediction: 63\n",
      "when input is [57, 43, 60, 43, 52, 1, 63] the prediction: 43\n",
      "when input is [57, 43, 60, 43, 52, 1, 63, 43] the prediction: 39\n",
      "when input is [60] the prediction: 43\n",
      "when input is [60, 43] the prediction: 42\n",
      "when input is [60, 43, 42] the prediction: 8\n",
      "when input is [60, 43, 42, 8] the prediction: 0\n",
      "when input is [60, 43, 42, 8, 0] the prediction: 25\n",
      "when input is [60, 43, 42, 8, 0, 25] the prediction: 63\n",
      "when input is [60, 43, 42, 8, 0, 25, 63] the prediction: 1\n",
      "when input is [60, 43, 42, 8, 0, 25, 63, 1] the prediction: 45\n",
      "when input is [56] the prediction: 42\n",
      "when input is [56, 42] the prediction: 5\n",
      "when input is [56, 42, 5] the prediction: 57\n",
      "when input is [56, 42, 5, 57] the prediction: 1\n",
      "when input is [56, 42, 5, 57, 1] the prediction: 57\n",
      "when input is [56, 42, 5, 57, 1, 57] the prediction: 39\n",
      "when input is [56, 42, 5, 57, 1, 57, 39] the prediction: 49\n",
      "when input is [56, 42, 5, 57, 1, 57, 39, 49] the prediction: 43\n",
      "when input is [43] the prediction: 57\n",
      "when input is [43, 57] the prediction: 58\n",
      "when input is [43, 57, 58] the prediction: 63\n",
      "when input is [43, 57, 58, 63] the prediction: 6\n",
      "when input is [43, 57, 58, 63, 6] the prediction: 1\n",
      "when input is [43, 57, 58, 63, 6, 1] the prediction: 58\n",
      "when input is [43, 57, 58, 63, 6, 1, 58] the prediction: 46\n",
      "when input is [43, 57, 58, 63, 6, 1, 58, 46] the prediction: 47\n",
      "when input is [43] the prediction: 1\n",
      "when input is [43, 1] the prediction: 51\n",
      "when input is [43, 1, 51] the prediction: 39\n",
      "when input is [43, 1, 51, 39] the prediction: 63\n",
      "when input is [43, 1, 51, 39, 63] the prediction: 1\n",
      "when input is [43, 1, 51, 39, 63, 1] the prediction: 40\n",
      "when input is [43, 1, 51, 39, 63, 1, 40] the prediction: 43\n",
      "when input is [43, 1, 51, 39, 63, 1, 40, 43] the prediction: 1\n",
      "when input is [58] the prediction: 46\n",
      "when input is [58, 46] the prediction: 43\n",
      "when input is [58, 46, 43] the prediction: 1\n",
      "when input is [58, 46, 43, 1] the prediction: 43\n",
      "when input is [58, 46, 43, 1, 43] the prediction: 39\n",
      "when input is [58, 46, 43, 1, 43, 39] the prediction: 56\n",
      "when input is [58, 46, 43, 1, 43, 39, 56] the prediction: 57\n",
      "when input is [58, 46, 43, 1, 43, 39, 56, 57] the prediction: 10\n",
      "when input is [39] the prediction: 58\n",
      "when input is [39, 58] the prediction: 47\n",
      "when input is [39, 58, 47] the prediction: 53\n",
      "when input is [39, 58, 47, 53] the prediction: 52\n",
      "when input is [39, 58, 47, 53, 52] the prediction: 12\n",
      "when input is [39, 58, 47, 53, 52, 12] the prediction: 1\n",
      "when input is [39, 58, 47, 53, 52, 12, 1] the prediction: 37\n",
      "when input is [39, 58, 47, 53, 52, 12, 1, 37] the prediction: 53\n",
      "when input is [53] the prediction: 56\n",
      "when input is [53, 56] the prediction: 43\n",
      "when input is [53, 56, 43] the prediction: 1\n",
      "when input is [53, 56, 43, 1] the prediction: 21\n",
      "when input is [53, 56, 43, 1, 21] the prediction: 1\n",
      "when input is [53, 56, 43, 1, 21, 1] the prediction: 41\n",
      "when input is [53, 56, 43, 1, 21, 1, 41] the prediction: 39\n",
      "when input is [53, 56, 43, 1, 21, 1, 41, 39] the prediction: 51\n",
      "when input is [50] the prediction: 39\n",
      "when input is [50, 39] the prediction: 52\n",
      "when input is [50, 39, 52] the prediction: 63\n",
      "when input is [50, 39, 52, 63] the prediction: 1\n",
      "when input is [50, 39, 52, 63, 1] the prediction: 47\n",
      "when input is [50, 39, 52, 63, 1, 47] the prediction: 58\n",
      "when input is [50, 39, 52, 63, 1, 47, 58] the prediction: 57\n",
      "when input is [50, 39, 52, 63, 1, 47, 58, 57] the prediction: 43\n",
      "when input is [56] the prediction: 53\n",
      "when input is [56, 53] the prediction: 63\n",
      "when input is [56, 53, 63] the prediction: 1\n",
      "when input is [56, 53, 63, 1] the prediction: 42\n",
      "when input is [56, 53, 63, 1, 42] the prediction: 47\n",
      "when input is [56, 53, 63, 1, 42, 47] the prediction: 42\n",
      "when input is [56, 53, 63, 1, 42, 47, 42] the prediction: 1\n",
      "when input is [56, 53, 63, 1, 42, 47, 42, 1] the prediction: 57\n",
      "when input is [39] the prediction: 51\n",
      "when input is [39, 51] the prediction: 1\n",
      "when input is [39, 51, 1] the prediction: 39\n",
      "when input is [39, 51, 1, 39] the prediction: 44\n",
      "when input is [39, 51, 1, 39, 44] the prediction: 56\n",
      "when input is [39, 51, 1, 39, 44, 56] the prediction: 39\n",
      "when input is [39, 51, 1, 39, 44, 56, 39] the prediction: 47\n",
      "when input is [39, 51, 1, 39, 44, 56, 39, 47] the prediction: 42\n",
      "when input is [17] the prediction: 24\n",
      "when input is [17, 24] the prediction: 21\n",
      "when input is [17, 24, 21] the prediction: 38\n",
      "when input is [17, 24, 21, 38] the prediction: 13\n",
      "when input is [17, 24, 21, 38, 13] the prediction: 14\n",
      "when input is [17, 24, 21, 38, 13, 14] the prediction: 17\n",
      "when input is [17, 24, 21, 38, 13, 14, 17] the prediction: 32\n",
      "when input is [17, 24, 21, 38, 13, 14, 17, 32] the prediction: 20\n",
      "when input is [1] the prediction: 39\n",
      "when input is [1, 39] the prediction: 52\n",
      "when input is [1, 39, 52] the prediction: 42\n",
      "when input is [1, 39, 52, 42] the prediction: 1\n",
      "when input is [1, 39, 52, 42, 1] the prediction: 45\n",
      "when input is [1, 39, 52, 42, 1, 45] the prediction: 43\n",
      "when input is [1, 39, 52, 42, 1, 45, 43] the prediction: 50\n",
      "when input is [1, 39, 52, 42, 1, 45, 43, 50] the prediction: 42\n",
      "when input is [1] the prediction: 58\n",
      "when input is [1, 58] the prediction: 46\n",
      "when input is [1, 58, 46] the prediction: 39\n",
      "when input is [1, 58, 46, 39] the prediction: 58\n",
      "when input is [1, 58, 46, 39, 58] the prediction: 1\n",
      "when input is [1, 58, 46, 39, 58, 1] the prediction: 42\n",
      "when input is [1, 58, 46, 39, 58, 1, 42] the prediction: 53\n",
      "when input is [1, 58, 46, 39, 58, 1, 42, 53] the prediction: 1\n",
      "when input is [1] the prediction: 61\n",
      "when input is [1, 61] the prediction: 53\n",
      "when input is [1, 61, 53] the prediction: 59\n",
      "when input is [1, 61, 53, 59] the prediction: 50\n",
      "when input is [1, 61, 53, 59, 50] the prediction: 42\n",
      "when input is [1, 61, 53, 59, 50, 42] the prediction: 1\n",
      "when input is [1, 61, 53, 59, 50, 42, 1] the prediction: 21\n",
      "when input is [1, 61, 53, 59, 50, 42, 1, 21] the prediction: 1\n",
      "when input is [59] the prediction: 57\n",
      "when input is [59, 57] the prediction: 40\n",
      "when input is [59, 57, 40] the prediction: 39\n",
      "when input is [59, 57, 40, 39] the prediction: 52\n",
      "when input is [59, 57, 40, 39, 52] the prediction: 42\n",
      "when input is [59, 57, 40, 39, 52, 42] the prediction: 1\n",
      "when input is [59, 57, 40, 39, 52, 42, 1] the prediction: 40\n",
      "when input is [59, 57, 40, 39, 52, 42, 1, 40] the prediction: 47\n",
      "when input is [52] the prediction: 42\n",
      "when input is [52, 42] the prediction: 8\n",
      "when input is [52, 42, 8] the prediction: 0\n",
      "when input is [52, 42, 8, 0] the prediction: 0\n",
      "when input is [52, 42, 8, 0, 0] the prediction: 23\n",
      "when input is [52, 42, 8, 0, 0, 23] the prediction: 21\n",
      "when input is [52, 42, 8, 0, 0, 23, 21] the prediction: 26\n",
      "when input is [52, 42, 8, 0, 0, 23, 21, 26] the prediction: 19\n",
      "when input is [45] the prediction: 53\n",
      "when input is [45, 53] the prediction: 42\n",
      "when input is [45, 53, 42] the prediction: 57\n",
      "when input is [45, 53, 42, 57] the prediction: 0\n",
      "when input is [45, 53, 42, 57, 0] the prediction: 23\n",
      "when input is [45, 53, 42, 57, 0, 23] the prediction: 43\n",
      "when input is [45, 53, 42, 57, 0, 23, 43] the prediction: 43\n",
      "when input is [45, 53, 42, 57, 0, 23, 43, 43] the prediction: 54\n",
      "when input is [52] the prediction: 1\n",
      "when input is [52, 1] the prediction: 61\n",
      "when input is [52, 1, 61] the prediction: 39\n",
      "when input is [52, 1, 61, 39] the prediction: 57\n",
      "when input is [52, 1, 61, 39, 57] the prediction: 1\n",
      "when input is [52, 1, 61, 39, 57, 1] the prediction: 51\n",
      "when input is [52, 1, 61, 39, 57, 1, 51] the prediction: 53\n",
      "when input is [52, 1, 61, 39, 57, 1, 51, 53] the prediction: 56\n",
      "when input is [39] the prediction: 49\n",
      "when input is [39, 49] the prediction: 12\n",
      "when input is [39, 49, 12] the prediction: 1\n",
      "when input is [39, 49, 12, 1] the prediction: 27\n",
      "when input is [39, 49, 12, 1, 27] the prediction: 1\n",
      "when input is [39, 49, 12, 1, 27, 1] the prediction: 58\n",
      "when input is [39, 49, 12, 1, 27, 1, 58] the prediction: 56\n",
      "when input is [39, 49, 12, 1, 27, 1, 58, 56] the prediction: 39\n",
      "when input is [53] the prediction: 44\n",
      "when input is [53, 44] the prediction: 1\n",
      "when input is [53, 44, 1] the prediction: 57\n",
      "when input is [53, 44, 1, 57] the prediction: 54\n",
      "when input is [53, 44, 1, 57, 54] the prediction: 43\n",
      "when input is [53, 44, 1, 57, 54, 43] the prediction: 43\n",
      "when input is [53, 44, 1, 57, 54, 43, 43] the prediction: 41\n",
      "when input is [53, 44, 1, 57, 54, 43, 43, 41] the prediction: 46\n",
      "when input is [57] the prediction: 53\n",
      "when input is [57, 53] the prediction: 52\n",
      "when input is [57, 53, 52] the prediction: 57\n",
      "when input is [57, 53, 52, 57] the prediction: 8\n",
      "when input is [57, 53, 52, 57, 8] the prediction: 0\n",
      "when input is [57, 53, 52, 57, 8, 0] the prediction: 0\n",
      "when input is [57, 53, 52, 57, 8, 0, 0] the prediction: 25\n",
      "when input is [57, 53, 52, 57, 8, 0, 0, 25] the prediction: 17\n",
      "when input is [1] the prediction: 42\n",
      "when input is [1, 42] the prediction: 43\n",
      "when input is [1, 42, 43] the prediction: 44\n",
      "when input is [1, 42, 43, 44] the prediction: 43\n",
      "when input is [1, 42, 43, 44, 43] the prediction: 41\n",
      "when input is [1, 42, 43, 44, 43, 41] the prediction: 58\n",
      "when input is [1, 42, 43, 44, 43, 41, 58] the prediction: 1\n",
      "when input is [1, 42, 43, 44, 43, 41, 58, 1] the prediction: 53\n",
      "when input is [21] the prediction: 1\n",
      "when input is [21, 1] the prediction: 61\n",
      "when input is [21, 1, 61] the prediction: 39\n",
      "when input is [21, 1, 61, 39] the prediction: 52\n",
      "when input is [21, 1, 61, 39, 52] the prediction: 42\n",
      "when input is [21, 1, 61, 39, 52, 42] the prediction: 43\n",
      "when input is [21, 1, 61, 39, 52, 42, 43] the prediction: 56\n",
      "when input is [21, 1, 61, 39, 52, 42, 43, 56] the prediction: 6\n",
      "when input is [43] the prediction: 43\n",
      "when input is [43, 43] the prediction: 51\n",
      "when input is [43, 43, 51] the prediction: 5\n",
      "when input is [43, 43, 51, 5] the prediction: 42\n",
      "when input is [43, 43, 51, 5, 42] the prediction: 1\n",
      "when input is [43, 43, 51, 5, 42, 1] the prediction: 40\n",
      "when input is [43, 43, 51, 5, 42, 1, 40] the prediction: 59\n",
      "when input is [43, 43, 51, 5, 42, 1, 40, 59] the prediction: 56\n",
      "when input is [45] the prediction: 50\n",
      "when input is [45, 50] the prediction: 63\n",
      "when input is [45, 50, 63] the prediction: 1\n",
      "when input is [45, 50, 63, 1] the prediction: 52\n",
      "when input is [45, 50, 63, 1, 52] the prediction: 53\n",
      "when input is [45, 50, 63, 1, 52, 53] the prediction: 61\n",
      "when input is [45, 50, 63, 1, 52, 53, 61] the prediction: 12\n",
      "when input is [45, 50, 63, 1, 52, 53, 61, 12] the prediction: 0\n",
      "when input is [52] the prediction: 53\n",
      "when input is [52, 53] the prediction: 58\n",
      "when input is [52, 53, 58] the prediction: 8\n",
      "when input is [52, 53, 58, 8] the prediction: 0\n",
      "when input is [52, 53, 58, 8, 0] the prediction: 25\n",
      "when input is [52, 53, 58, 8, 0, 25] the prediction: 63\n",
      "when input is [52, 53, 58, 8, 0, 25, 63] the prediction: 1\n",
      "when input is [52, 53, 58, 8, 0, 25, 63, 1] the prediction: 61\n",
      "when input is [53] the prediction: 58\n",
      "when input is [53, 58] the prediction: 6\n",
      "when input is [53, 58, 6] the prediction: 1\n",
      "when input is [53, 58, 6, 1] the prediction: 51\n",
      "when input is [53, 58, 6, 1, 51] the prediction: 63\n",
      "when input is [53, 58, 6, 1, 51, 63] the prediction: 1\n",
      "when input is [53, 58, 6, 1, 51, 63, 1] the prediction: 50\n",
      "when input is [53, 58, 6, 1, 51, 63, 1, 50] the prediction: 53\n",
      "when input is [52] the prediction: 57\n",
      "when input is [52, 57] the prediction: 8\n",
      "when input is [52, 57, 8] the prediction: 0\n",
      "when input is [52, 57, 8, 0] the prediction: 21\n",
      "when input is [52, 57, 8, 0, 21] the prediction: 5\n",
      "when input is [52, 57, 8, 0, 21, 5] the prediction: 50\n",
      "when input is [52, 57, 8, 0, 21, 5, 50] the prediction: 50\n",
      "when input is [52, 57, 8, 0, 21, 5, 50, 50] the prediction: 1\n",
      "when input is [47] the prediction: 58\n",
      "when input is [47, 58] the prediction: 46\n",
      "when input is [47, 58, 46] the prediction: 1\n",
      "when input is [47, 58, 46, 1] the prediction: 63\n",
      "when input is [47, 58, 46, 1, 63] the prediction: 53\n",
      "when input is [47, 58, 46, 1, 63, 53] the prediction: 59\n",
      "when input is [47, 58, 46, 1, 63, 53, 59] the prediction: 6\n",
      "when input is [47, 58, 46, 1, 63, 53, 59, 6] the prediction: 1\n",
      "when input is [1] the prediction: 53\n",
      "when input is [1, 53] the prediction: 40\n",
      "when input is [1, 53, 40] the prediction: 43\n",
      "when input is [1, 53, 40, 43] the prediction: 63\n",
      "when input is [1, 53, 40, 43, 63] the prediction: 12\n",
      "when input is [1, 53, 40, 43, 63, 12] the prediction: 0\n",
      "when input is [1, 53, 40, 43, 63, 12, 0] the prediction: 26\n",
      "when input is [1, 53, 40, 43, 63, 12, 0, 26] the prediction: 39\n"
     ]
    }
   ],
   "source": [
    "#working with multiple chunks/batchs together\n",
    "#working with batch dimension \n",
    "\n",
    "#random number generator/changes this \n",
    "torch.manual_seed(1337) \n",
    "\n",
    "\n",
    "def batch(input):\n",
    "    if input=='train':\n",
    "        data=train_data\n",
    "    else: \n",
    "        data=test_data\n",
    "    index=torch.randint(len(data)-sequence_length, (batch_size,)) #grabbing head_size number of random offsets/index is going to be #defined by head_size randomly generated between the length of data and our contextual_length\n",
    "    contextual=torch.stack([data[i:i+sequence_length] for i in index]) #all become a row in a 4 by 8 tensor \n",
    "    prediction=torch.stack([data[i+1:i+sequence_length+1] for i in index])\n",
    "    contextual, prediction=contextual.to(device), prediction.to(device)\n",
    "    return contextual, prediction\n",
    "\n",
    "train_context, train_labels=batch('train')\n",
    "print('inputs:')\n",
    "print(train_context.shape)\n",
    "print(train_context)\n",
    "print('prediction:')\n",
    "print(train_labels.shape)\n",
    "print(train_labels)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): #number of heads\n",
    "    for c in range(contextual_length): \n",
    "        context=train_context[b, :c+1]\n",
    "        prediction=train_labels[b, c]\n",
    "        print(f\"when input is {context.tolist()} the prediction: {prediction}\") \n",
    "\n",
    "\n",
    "#we are getting 4 rows (represent the heads) \n",
    "#we get 8 columns (represent the contextual part of it)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e77cc-ba17-4c7c-9033-a433ab324309",
   "metadata": {},
   "source": [
    "# bigram language model \n",
    "- looks at a pair of words instead of just one just one word at a time\n",
    "- looks to see how often each pair of words appear together\n",
    "- this helps figure out the likelihood of certain words following each other\n",
    "- a simple way to understand and predict the next word in a sentence\n",
    "- another possible model to look into are n-grams\n",
    "- this model helps in various autocorrect, text generation, and speech recognition models\n",
    "- nn module is deep learning framework/providdes a collection of classes and functions that make it easier to define and train neural networks\n",
    "\n",
    "### Constructor\n",
    "- embeddings are a way to represent categorical varialbes as vectors \n",
    "- nn.embedding= specificies the number of unique categories (the size of the vocab in NLP) and desired size of the embedding vectors (a hyperparameter that determines the length of the embedding vector/eg if you have a vocab size of 10000 and embedding dimension of 300, this means that each word in the vocab will be represented as a 300-dim vector/smaller values are used when computational resources are limited and larer values might be preferred for more complex tasks):\n",
    "     - input: an index of sequence of indices that represent the category you want to convert into an embedding (these indices could be word IDs where each word in the vocab is mapped to a unique integer)\n",
    "     - output: the dense representation (embedding vector) of the input index. The output is a tensor with shape('batch size, embedding_dim)\n",
    " \n",
    "### Forward Method \n",
    "- self.token_embedding_table(index): an embedding lookup for each index in the index tensor occurs. It retrieves the embedding from lookup table/return value is a tensor where each word in index has been converted into a desne vector representation\n",
    "- if predictions is None: this will check whether we are training the model or not/if we are training then no loss calculation is required as the model is generating predictions\n",
    "- if predictions is provided, this means that the model is being used for training and we calculate the loss using the cross-entropy function\n",
    "- B, T, C:\n",
    "    - B: Batch size\n",
    "    - T: sequence length\n",
    "    - C: embedding Dimension\n",
    "- our_pred=our_pred.view(B*T, C):\n",
    "    - breaks down our_pred into a 2-dimensional tensor \n",
    "    - this is done in preparation for the corss-entropy loss function\n",
    "- prediction.view(B*T):\n",
    "    - the prediction tensor is re-shaped into a 1 dimensional tensor\n",
    "    - this is essnetial for matching the shapes of our_pred and prediction for entropy loss function\n",
    "    - it expects the total number of elements in the batch \n",
    "- F.cross_entropy(our_pred, predictions)\n",
    "    - the function is provided by PyTorch's functional interface\n",
    "    - this function is particularly used in classification tasks\n",
    "    - calculates the difference between predicted probability distribution and true probability distribution of the target lables\n",
    "    - the goal of the cross-entropy loss is to miniize the dissimilarity between these distributions, and improve model accuracy\n",
    "    - if the model's predicted probability for the true class is close to 1 (high confidence), the cross entropy loss will be close to zero\n",
    "    - inputs:\n",
    "         - Input tensor: representes the predicted raw scores from the model. It should be a 2-dimesnional tensor with shape (B, C), where    B=batch size and C is the number of classes/vocab size\n",
    "         - Target tensor: represente the true class labels for each sample in the batch. It should be 1-dimensional tensor with shape (B), where B is the batch size/total number of elements in the batch=\n",
    "    - outputs: a scaler value that represents the average loss across the batch\n",
    " \n",
    "This may look like: \n",
    "- input_tensor=torch.tensor([[0.2, 0.5, 0.3], [1.2, 0.1,-0.2], [-0.5, 0.2,1.2], [0.1,0.2, 0.7]]]): batch_size=4, number of class/vocab size=3\n",
    "- target_tensor=torch.tensor([1, 0, 2, 0])\n",
    "\n",
    "\n",
    "### Generate Method \n",
    "- index is an array of indices for the current context/it is (B,T) array of Batch size and sequence length (number of tokens in the context)\n",
    "- aim is to extent to be (B, T+1), (B,T+2), (B,T+3)...../continues generation until we hit the end of the sequence length\n",
    "- it will do this for new_tokens\n",
    "- we call .forward method, which computes the next token prediction\n",
    "- we then slice our_pred: [:, -1,:]: we want to include all of the batch dimension/vocab_size, but we only want the last element (so we slice our sequence length to get the last element)/after slicing, we a re left with a tensor with a new shape of (B,C)/slicing effectively removes the second dimension (T)/we do this to focus on th e logits for the last time step \n",
    "- we convert to probabilities with the soft_max function is applied to convert the into probability scores/softmax function scales our_pred so that they will sum to 1/this produces a tensor of shape (B,C) representing the probabilities of each token in the vocabulary being the next token in the sequence/each element in probabilities represet the probabilitity of the token being the next token in the sequence\n",
    "     - the dimension parameter determines the dimension the softmax operaiton is applied/it specifies the axis of the tensor on which the sum of exponential is calculated. The result is then normalized by dividing each element by the sum of the exponentials along that axis\n",
    "     - dim=-1/is because we want the last dimension of the tensor, so probability distribution over the vocabulary/this calculates the probabilities for each batch sample seperatly, treating the tokens in the vocabulary as indpendent classes for each sample \n",
    "- torch.multinomial samples from those probabilities/we want one sample/used to sample a token index from the probabilitiy distribution:\n",
    "    - probabilitiesL the probability distribution\n",
    "    - samples_num: how many tokens to draw from the multinomial distribution for each batch element/we want 1 token for each batch element\n",
    "    - the randomly sampled tokens is crucial for generating diverse and unpredictable sequences in a language model or text generation task\n",
    "    - we get back the tensor (B,1): a tensor containing the sampled indices of the outcomes for each element in the batch \n",
    "         - contains the sampled token indices for each sequence in the batch\n",
    "         - the elements in next_index tensor are integers that represent the indices of the tokens sampled from the probabilitiy distribution\n",
    "         - For example:\n",
    "              - if probs=torch.tensor([[0.2,0.3,0.1, 0.15, 0.25], [0.1, 0.05, 0.4, 0.15, 0.3]]) (these represent sample probabilities for each sequence in the batch\n",
    "              - next_index=torch.multinomial(probs, num_samples=1)\n",
    "              - if we were to print next_index we would get a tensor of shape tensor([4], [2]]): this rempresents the index of the sample token in the batch (the tensor has the shape (2,1))\n",
    "          \n",
    "- index=torch.cat((index, next_index), dimension=1):\n",
    "          - has the effect of extending the existing context tensor\n",
    "  \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "faba3d2e-92bb-43eb-a9ed-a9a8386dd8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([280, 65])\n",
      "tensor(4.2175, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [1, 8] but got: [1, 1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[198], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss) \u001b[38;5;66;03m#we expecting loss to be 4.17/we are guessing wrong/getting some entropy  \u001b[39;00m\n\u001b[1;32m     61\u001b[0m index\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;66;03m#feeding it a tensor of (1,1) that holds a zero and it how we kick of the generate function with type being integer/represents an empty context to start the sequence generation\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoder(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())) \u001b[38;5;66;03m#we call the generate method with our empty context/we want to generate 300 characters here/we extract the first element of the tuple (this is because the generate function returns a tuple of (predicted value, loss)/we want predicted value \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[198], line 47\u001b[0m, in \u001b[0;36mBigramLM.generate\u001b[0;34m(self, index, new_tokens)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(new_tokens):\n\u001b[1;32m     46\u001b[0m     condition_index\u001b[38;5;241m=\u001b[39mindex[:, \u001b[38;5;241m-\u001b[39msequence_length:]\n\u001b[0;32m---> 47\u001b[0m     our_pred, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     our_pred\u001b[38;5;241m=\u001b[39mour_pred[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     49\u001b[0m     probability\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39msoftmax(our_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[198], line 31\u001b[0m, in \u001b[0;36mBigramLM.forward\u001b[0;34m(self, index, predictions)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#gives us our predictions, but be careful because the c in the token_emb is different from our_pred c \u001b[39;00m\n\u001b[1;32m     30\u001b[0m y\u001b[38;5;241m=\u001b[39mtoken_embd\u001b[38;5;241m+\u001b[39mpos_embd \n\u001b[0;32m---> 31\u001b[0m y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_sa\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m                            \n\u001b[1;32m     32\u001b[0m our_pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_lm(y)  \u001b[38;5;66;03m#(B,T, vocab_size)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predictions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[175], line 20\u001b[0m, in \u001b[0;36mHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m weights\u001b[38;5;241m=\u001b[39mweights\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtril[:T, :T]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     19\u001b[0m weights\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39msoftmax(weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m output\u001b[38;5;241m=\u001b[39m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [1, 8] but got: [1, 1]."
     ]
    }
   ],
   "source": [
    "#reproducability \n",
    "torch.manual_seed(1337)\n",
    "\n",
    "#Big gram language model \n",
    "#inherits from nn module  \n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "\n",
    "    #constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table=nn.Embedding(size_vocab, dim_embd)\n",
    "        #positional encoding\n",
    "        self.pos_embedding_table=nn.Embedding(sequence_length, dim_embd) \n",
    "        #creating a self-attention head\n",
    "        self.head_sa=Head(dim_embd)\n",
    "        #creating a linear layer \n",
    "        self.head_lm=nn.Linear(dim_embd, size_vocab)\n",
    "\n",
    "    def forward(self, index, predictions=None):\n",
    "        B,T=index.shape\n",
    "\n",
    "        #gives us token embeddings \n",
    "        token_embd=self.token_embedding_table(index) #(B,T,C)/this is the identity of the token \n",
    "\n",
    "        #we need a positional encoding \n",
    "        pos_embd=self.pos_embedding_table(torch.arange(T, device=device)) #this is the position of the token  \n",
    "        \n",
    "        #gives us our predictions, but be careful because the c in the token_emb is different from our_pred c \n",
    "        y=token_embd+pos_embd \n",
    "        y=self.head_sa(y)                            \n",
    "        our_pred=self.head_lm(y)  #(B,T, vocab_size)\n",
    "        \n",
    "        \n",
    "        if predictions is None:\n",
    "            loss=None \n",
    "        else:\n",
    "            B, T, C=our_pred.shape\n",
    "            our_pred=our_pred.view(B*T,C)\n",
    "            predictions=predictions.view(B*T)\n",
    "            loss=F.cross_entropy(our_pred, predictions)\n",
    "        return our_pred, loss\n",
    "        \n",
    "    def generate(self, index, new_tokens):\n",
    "        for _ in range(new_tokens):\n",
    "            condition_index=index[:, -sequence_length:]\n",
    "            our_pred, loss=self.forward(condition_index)\n",
    "            our_pred=our_pred[:, -1, :]\n",
    "            probability=F.softmax(our_pred, dim=-1)\n",
    "            next_index=torch.multinomial(probability, num_samples=1)\n",
    "            index=torch.cat((index,next_index), dim=1)\n",
    "        return index \n",
    "\n",
    "\n",
    "\n",
    "model=BigramLM()\n",
    "model=model.to(device)\n",
    "output, loss=model.forward(train_context, train_labels)\n",
    "print(output.shape)\n",
    "print(loss) #we expecting loss to be 4.17/we are guessing wrong/getting some entropy  \n",
    "index=torch.zeros((1,1), dtype=torch.long, device=device) #feeding it a tensor of (1,1) that holds a zero and it how we kick of the generate function with type being integer/represents an empty context to start the sequence generation\n",
    "print(decoder(model.generate(index, new_tokens=300)[0].tolist())) #we call the generate method with our empty context/we want to generate 300 characters here/we extract the first element of the tuple (this is because the generate function returns a tuple of (predicted value, loss)/we want predicted value \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "918623ae-665a-41b4-a7e7-adeb458c0824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@torch.no_grad() tells pytorch we don't intend to do back propogation: doesn't save intermediate variables \n",
    "#averages out the loss over multiple batches \n",
    "#we do this iters_eval times and get the loss for both splits \n",
    "\n",
    "@torch.no_grad()\n",
    "def loss_estimator(): \n",
    "    output={}\n",
    "    model.eval()\n",
    "    for type in ['train', 'test']:\n",
    "        lossed=torch.zeros(iters_eval)\n",
    "        for x in range(iters_eval):\n",
    "            contextual, labeled=batch(type) \n",
    "            our_pred, loss=model(contextual, labeled)\n",
    "            lossed[x]=loss.item()\n",
    "        output[type]=lossed.mean()\n",
    "    model.train()\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f90505-72dd-4840-9f65-622c16ccf4db",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e2e669-d0ec-41ba-8e61-faf4163f7c20",
   "metadata": {},
   "source": [
    "## Notes: \n",
    "- an optimizer object is an essnetial componenet used during the trianing of machine learning models, specifically neural networks\n",
    "- it is responsible for updating the parameters durin the training process to minimize the loss function\n",
    "- there are various optimizer classes that implement different optimization algorithms, such as Adam, SGD\n",
    "- To use optimizer:\n",
    "  - create an instance of the optimizer (what we did below) \n",
    "  - training loop:\n",
    "       1. forward pass (pass the input data through our model to get predictions\n",
    "       2.  calculate the loss (using predicted values and true labels (what we did with nn.CrossEntropyLoss())\n",
    "       3.  backward pass (calculate gradient of the loss with respect to model paramaters)\n",
    "       4.  update paramters: the optimizer updates the model's parameters based on the gradients and optimization algorithm\n",
    "   \n",
    "### Forward Pass \n",
    "- num_epochs: a hyperparameter that represents the total number of times the training dataset is passed through the learning algorithm during the training process/it is basically the number of times the model will see the entire training dataset and update its parameters (weights/biases) accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d326dbcc-d5ac-429b-94f6-9bce1a9ce07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an optimizer object\n",
    "optimizer=torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b188ceec-db9a-40a6-8dcc-308a2bb57771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.198831081390381, test loss 4.202894687652588\n",
      "step 500: train loss 4.200320243835449, test loss 4.2017645835876465\n",
      "step 1000: train loss 4.199172496795654, test loss 4.200868129730225\n",
      "step 1500: train loss 4.200717449188232, test loss 4.202966213226318\n",
      "step 2000: train loss 4.200040340423584, test loss 4.205711841583252\n",
      "step 2500: train loss 4.200659275054932, test loss 4.202916145324707\n",
      "step 3000: train loss 4.201669216156006, test loss 4.202471733093262\n",
      "step 3500: train loss 4.199456691741943, test loss 4.2001214027404785\n",
      "step 4000: train loss 4.19920015335083, test loss 4.202147006988525\n",
      "step 4500: train loss 4.199309825897217, test loss 4.201878547668457\n",
      "4.203585147857666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#forward loop \n",
    "\n",
    "for iter in range(num_epochs):\n",
    "    if iter % interval_eval==0:\n",
    "        lossed=loss_estimator()\n",
    "        print(f\"step {iter}: train loss {lossed['train']}, test loss {lossed['test']}\")\n",
    "\n",
    "    \n",
    "    #sample our batch of data \n",
    "    contextual, correct_labels=batch('train')\n",
    "    #forward pass/including the loss calculation \n",
    "    our_pred, loss=model(contextual, correct_labels)\n",
    "    #backward pass \n",
    "    optimizer.zero_grad(set_to_none=True) #getting the gradients for all our parameters \n",
    "    loss.backward() #using those gradients to update our parameters \n",
    "    #update our values \n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c7649da6-7d90-4af7-8faf-329d0e617ff1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [1, 8] but got: [1, 1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoder(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())) \u001b[38;5;66;03m#we call the generate method with our empty context/we want to generate 300 characters here/we extract the first element of the tuple (this is because the generate function returns a tuple of (predicted value, loss)/we want predicted value \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[177], line 47\u001b[0m, in \u001b[0;36mBigramLM.generate\u001b[0;34m(self, index, new_tokens)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(new_tokens):\n\u001b[1;32m     46\u001b[0m     condition_index\u001b[38;5;241m=\u001b[39mindex[:, \u001b[38;5;241m-\u001b[39mblock_size:]\n\u001b[0;32m---> 47\u001b[0m     our_pred, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     our_pred\u001b[38;5;241m=\u001b[39mour_pred[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     49\u001b[0m     probability\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39msoftmax(our_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[177], line 31\u001b[0m, in \u001b[0;36mBigramLM.forward\u001b[0;34m(self, index, predictions)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#gives us our predictions, but be careful because the c in the token_emb is different from our_pred c \u001b[39;00m\n\u001b[1;32m     30\u001b[0m y\u001b[38;5;241m=\u001b[39mtoken_embd\u001b[38;5;241m+\u001b[39mpos_embd \n\u001b[0;32m---> 31\u001b[0m y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_sa\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m                            \n\u001b[1;32m     32\u001b[0m our_pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(y)  \u001b[38;5;66;03m#(B,T, vocab_size)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predictions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[175], line 20\u001b[0m, in \u001b[0;36mHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m weights\u001b[38;5;241m=\u001b[39mweights\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtril[:T, :T]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     19\u001b[0m weights\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39msoftmax(weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m output\u001b[38;5;241m=\u001b[39m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [1, 8] but got: [1, 1]."
     ]
    }
   ],
   "source": [
    "print(decoder(model.generate(index, new_tokens=800)[0].tolist())) #we call the generate method with our empty context/we want to generate 300 characters here/we extract the first element of the tuple (this is because the generate function returns a tuple of (predicted value, loss)/we want predicted value \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085f614-3695-4854-97a6-f5a73d207973",
   "metadata": {},
   "source": [
    "## Self-Attention: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8951c67-2dee-4733-b0ad-980c57e916b6",
   "metadata": {},
   "source": [
    "- torch.trial: returns the lower triangle part of a matrix/prevents the matrix from using tokens in front of it/can only get context\n",
    "- we can get the average of rows of a matrix by:\n",
    "   1. take our matrix a/torch.sum(a, 1, keepdim=True): basically ensuring that each row in A adds up to 1\n",
    "   2. we can then multiple a with another matrix to get matrix c\n",
    "   3. matrix c will give us a matrix with the rows being averaged from a/b\n",
    " \n",
    "Explaination of version with soft-max: \n",
    "- tril=torch.tril(torch.ones(T,T)): creates a square matrix of T x T filled with ones/torch.tril sets all elements above the main diagonal to zero\n",
    "- weights=torch.zeros((T,T)): create another square matrix of Size T x T filled with zeros/used to store weights that determine how much information from each token should be aggregated\n",
    "- weights=weights.masked_fill(tril==0, float('-inf')) we modify the weights matrix. basically we look at when tril==0, which occurs outside of the diagonal triangle matrix and set those to -inf-> results in zeros from the downward triangle matrix and a bunch of -inf that aren't a part of it\n",
    "- F.softmax: apply to the weights matrix along the last dimension because weights matrix is size Tx T, the last dimension is the column dimension/some tokens find others more or more interesting\n",
    "- After .softmax we get the weights matrix which contains probabilities for aggregating information from each token\n",
    "- every single token will admit two vectors: query (what am i looking for), key vector (what I contain):\n",
    "   - my query dot products with key vector and that becomes weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ad90009b-d881-4c23-91f2-c5769ed05b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example \n",
    "torch.manual_seed(1337)\n",
    "B,T,C=4, 8, 32\n",
    "x=torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9089b7c0-9295-438f-9107-0fb62317a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbagofwords=torch.zeros((B,T,C))\n",
    "for v in range(B):\n",
    "    for y in range(T):\n",
    "        prev=x[v,:y+1] #at this batch dimension/everything up to and including the t toke/slicing x to be shaped (t,C)-> keeps all the channels in-tact\n",
    "        xbagofwords[v,y]=torch.mean(prev, 0) #we are averaging out the time/which gives us a 1-dimensional vector C/we store it in xbagofwords\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6559d8d8-eff0-49ba-91d4-7715ece8cd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=torch.tril(torch.ones(T,T))\n",
    "weights=weights/torch.sum(weights,1, keepdim=True)\n",
    "xbagofwords2=weights@x #multiplying the matrix weights with our matrix x  (B,T,T) @ (B,T,C)->(B,T,C) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11f9dd61-b664-4a35-aefd-2ffff6831bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Version with softmax\n",
    "\n",
    "tril=torch.tril(torch.ones(T,T))\n",
    "weights=torch.zeros((T,T)) #weights begin with zero/how much from each token do we want to aggregate\n",
    "weights=weights.masked_fill(tril==0, float('-inf')) #tokens from the past cannot communicate\n",
    "weights=F.softmax(weights, dim=-1) #normalization/exponenetiate and then divde by sum/how much each element fuses into this position\n",
    "xbagofwords3=weights@x \n",
    "weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6b58009e-f40c-4af7-8e9b-93a7686ca2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single head of self attention \n",
    "torch.manual_seed(1337) \n",
    "B,T,C=4,8,32\n",
    "x=torch.randn(B,T,C)\n",
    "\n",
    "size_head=16\n",
    "query=nn.Linear(C, size_head, bias=False)\n",
    "key=nn.Linear(C, size_head, bias=False)\n",
    "value=nn.Linear(C, size_head, bias=False) \n",
    "v=value(x)\n",
    "q=query(x) # (B,T, 16)\n",
    "k=key(x) #(B,T,16)\n",
    "weights=q@k.transpose(-2,-1) #we need transpose the last two dimension (B,T,16) @ (B, 16, T) -> (B, T,T)\n",
    "\n",
    "tril=torch.tril(torch.ones(T,T))\n",
    "weights=torch.zeros((T,T)) #weights begin with zero/how much from each token do we want to aggregate\n",
    "weights=weights.masked_fill(tril==0, float('-inf')) #tokens from the past cannot communicate\n",
    "weights=F.softmax(weights, dim=-1) #normalization/exponenetiate and then divde by sum/how much each element fuses into this position\n",
    "output=weights@v\n",
    "output.shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e1483948-cd26-47c2-b974-80bc2ac6c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single-head of self Attention \n",
    "\n",
    "class Head(nn.Module): \n",
    "    def __init__(self, size_head):\n",
    "        super().__init__()\n",
    "        self.query=nn.Linear(dim_embd, size_head, bias=False) \n",
    "        self.key=nn.Linear(dim_embd, size_head, bias=False)\n",
    "        self.value=nn.Linear(dim_embd, size_head, bias=False)\n",
    "        #no a paramteter of the model, so in py-torch we have to call it a register_buffer\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(sequence_length, sequence_length))) #lower triangular matrix \n",
    "\n",
    "    def forward(self, x):\n",
    "        q=self.query(x)\n",
    "        k=self.key(x)\n",
    "        v=self.value(x)\n",
    "        #we now compute the self-attention scores \n",
    "        weights=q@k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
    "        weights=weights.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "        weights=F.softmax(weights, dim=-1)\n",
    "        output=weights @ v\n",
    "        return output \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6542e-d05a-4a1c-bf9f-6a41548805d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
