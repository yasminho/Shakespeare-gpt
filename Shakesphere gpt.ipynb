{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bff913a6-4a83-4a55-af0f-74305d64161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my imports\n",
    "import pandas as pd\n",
    "#requests is a popular python libarary for making HTTP requests, such as downloading from URLs\n",
    "import requests\n",
    "import urllib.request\n",
    "import sentencepiece as spm\n",
    "import tiktoken \n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7fc04c-6f69-421e-848c-28f5793a1929",
   "metadata": {},
   "source": [
    "## My Variables \n",
    "1. vocab size=size of unique characters\n",
    "2. characters=the sorted list of unique characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d0c428b9-1341-41b3-9b86-f4b839a9280a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#got the url by open the raw version of the file in github \n",
    "url = \"https://raw.githubusercontent.com/yasminho/Shakespeare-gpt/main/input.txt?token=GHSAT0AAAAAACDEOEB7HCMDF43ROSZND5FAZF6U3OA\"\n",
    "try:\n",
    "    # Download the dataset using urllib\n",
    "    #downloads the contents from url and saves it as input.txt \n",
    "    urllib.request.urlretrieve(url, 'input.txt')\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "#if there is an exception, the code will enter the exception block \n",
    "except Exception as e:\n",
    "    print(f\"the dataset did not download: {e}\")\n",
    "\n",
    "#will enter the exception block and print the exception "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b9ec3cd5-5a00-4a8f-9f3e-ee839194eb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "#reading in the dataset \n",
    "#'r' means read \n",
    "#common encoding for text files \n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "\n",
    "print(len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "22392cf8-7fff-4184-aeee-1650fd323f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a34b3342-dd04-43fb-aded-15b9ea2ea41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "#list of the set of characters in the text \n",
    "characters=sorted(list(set(text)))\n",
    "#number of unique characters in the text \n",
    "size_vocab=len(characters)\n",
    "\n",
    "print(size_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50f75e-585f-4438-888c-7972772c3c5f",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8d8aa9d8-3ca4-43e2-b013-b5e4b24bb906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 47, 1, 37, 39, 57]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a mappings\n",
    "#dictionary mapping characters to corresponding integers using enumerate\n",
    "#enumerate returns an iterator that produces tuples containing index and the element from input sequence \n",
    "chars_int_mapping={char:int for int, char in enumerate(characters)}\n",
    "#dictionary mpping integers to corresponding characters \n",
    "int_chars_mapping={int: char for int, char in enumerate(characters)}\n",
    "#takes in a string and encodes it into a list of integers using our mapping/converts each character in string to corresponding integer value and returns list of integers\n",
    "encoder=lambda str:[chars_int_mapping[char] for char in str]\n",
    "#takes in a list of integers and decoes it to a string using our int_chars_mapping\n",
    "decoder=lambda int_list: ''.join([int_chars_mapping[i] for i in int_list])\n",
    "\n",
    "#when we use this encoder, we will get a list of numbers between 0-65 because that is the size of vocabulary \n",
    "#notice that we get a token for each character\n",
    "encoder(\"Hi Yas\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4001b4af-f89a-431d-b6e0-b4eacbee87eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17250]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizing with tiktoken \n",
    "tik_token_encoder=tiktoken.get_encoding('gpt2')\n",
    "#instead of 65 tokens (our original vocabulary size), it has 50257 tokens \n",
    "#when we use this encoder \n",
    "tik_token_encoder.n_vocab\n",
    "\n",
    "#when we use tik_token_encoder, we get a number that anywhere from 0-50257\n",
    "#notice that we get a token for each word \n",
    "tik_token_encoder.encode(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bbbea3ef-1d2c-464f-a939-12a006171756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "#encoding the text dataset and into a torch.tensor\n",
    "#torch.tensor is a multi-dimensional array \n",
    "#takes in a list or array and converts into a pytorch tensor\n",
    "#the dtype=torch.long argument tells us the specified data type of the elements in the tensor, which is long (usually for integer data)\n",
    "encoded_data=torch.tensor(encoder(text), dtype=torch.long)\n",
    "print(encoded_data.dtype)\n",
    "print(encoded_data.shape)\n",
    "print(encoded_data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b4adaf-78ae-443a-bde7-475ccf2da97a",
   "metadata": {},
   "source": [
    "# splitting data\n",
    "- splitting into train and test set\n",
    "- 80% will train and 20% will test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a53b335c-3faf-4a8b-ae76-5bd30d3b2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look into nested-cross-validation\n",
    "train_length=int(len(encoded_data)*0.80)\n",
    "train_data=encoded_data[:train_length]\n",
    "test_data=encoded_data[train_length:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e19448-3257-43bd-9064-be66c6e841a4",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "91d694ab-6f84-415a-bb78-09fa935e570a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we only work with chunks of the data and train it at chunks at a time \n",
    "#the maximum length-called block_size \n",
    "#all of these characters follow each other, simulatenosouly make a prediction at all of them \n",
    "#8 individual examples: when you are trying to predict 9, we have 8 examples \n",
    "\n",
    "context_length=8 \n",
    "train_data[:context_length+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fb0c9e8c-8b96-451e-8450-44cf851733fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the prediction: 47\n",
      "when input is tensor([18, 47]) the prediction: 56\n",
      "when input is tensor([18, 47, 56]) the prediction: 57\n",
      "when input is tensor([18, 47, 56, 57]) the prediction: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the prediction: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the prediction: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the prediction: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the prediction: 58\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x=train_data[:context_length]\n",
    "#y includes one after x\n",
    "#y is the target for each position/the idea that we can predict the second element based on only one input \n",
    "y=train_data[1: context_length+1]\n",
    "\n",
    "#iterating up to context_length because the y: includes the last position\n",
    "\n",
    "for z in range(context_length):\n",
    "    #up to and including z/array x is from 0-block_size\n",
    "    contextual=x[:z+1]\n",
    "    #not including z/array y is from 1-block_size+1\n",
    "    prediction=y[z]\n",
    "    print(f\"when input is {contextual} the prediction: {prediction}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "90c93a07-c88d-4439-8b89-827ef5ebc808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[58, 63,  8,  0,  0, 19, 24, 27],\n",
      "        [39, 59, 45, 46, 58,  1, 46, 43],\n",
      "        [49, 43, 57,  1, 53, 50, 42,  1],\n",
      "        [52, 41, 47, 43, 52, 58,  1, 56]])\n",
      "prediction:\n",
      "torch.Size([4, 8])\n",
      "tensor([[63,  8,  0,  0, 19, 24, 27, 33],\n",
      "        [59, 45, 46, 58,  1, 46, 43,  1],\n",
      "        [43, 57,  1, 53, 50, 42,  1, 46],\n",
      "        [41, 47, 43, 52, 58,  1, 56, 47]])\n",
      "----\n",
      "when input is [58] the prediction: 63\n",
      "when input is [58, 63] the prediction: 8\n",
      "when input is [58, 63, 8] the prediction: 0\n",
      "when input is [58, 63, 8, 0] the prediction: 0\n",
      "when input is [58, 63, 8, 0, 0] the prediction: 19\n",
      "when input is [58, 63, 8, 0, 0, 19] the prediction: 24\n",
      "when input is [58, 63, 8, 0, 0, 19, 24] the prediction: 27\n",
      "when input is [58, 63, 8, 0, 0, 19, 24, 27] the prediction: 33\n",
      "when input is [39] the prediction: 59\n",
      "when input is [39, 59] the prediction: 45\n",
      "when input is [39, 59, 45] the prediction: 46\n",
      "when input is [39, 59, 45, 46] the prediction: 58\n",
      "when input is [39, 59, 45, 46, 58] the prediction: 1\n",
      "when input is [39, 59, 45, 46, 58, 1] the prediction: 46\n",
      "when input is [39, 59, 45, 46, 58, 1, 46] the prediction: 43\n",
      "when input is [39, 59, 45, 46, 58, 1, 46, 43] the prediction: 1\n",
      "when input is [49] the prediction: 43\n",
      "when input is [49, 43] the prediction: 57\n",
      "when input is [49, 43, 57] the prediction: 1\n",
      "when input is [49, 43, 57, 1] the prediction: 53\n",
      "when input is [49, 43, 57, 1, 53] the prediction: 50\n",
      "when input is [49, 43, 57, 1, 53, 50] the prediction: 42\n",
      "when input is [49, 43, 57, 1, 53, 50, 42] the prediction: 1\n",
      "when input is [49, 43, 57, 1, 53, 50, 42, 1] the prediction: 46\n",
      "when input is [52] the prediction: 41\n",
      "when input is [52, 41] the prediction: 47\n",
      "when input is [52, 41, 47] the prediction: 43\n",
      "when input is [52, 41, 47, 43] the prediction: 52\n",
      "when input is [52, 41, 47, 43, 52] the prediction: 58\n",
      "when input is [52, 41, 47, 43, 52, 58] the prediction: 1\n",
      "when input is [52, 41, 47, 43, 52, 58, 1] the prediction: 56\n",
      "when input is [52, 41, 47, 43, 52, 58, 1, 56] the prediction: 47\n"
     ]
    }
   ],
   "source": [
    "#working with multiple chunks/batchs together\n",
    "#working with batch dimension \n",
    "\n",
    "#random number generator/changes this \n",
    "torch.manual_seed(1337) \n",
    "#number of different independent sequences that we will process in parallel \n",
    "head_size=4 \n",
    "#maximum contextual length for the predictions \n",
    "contextual_length=8\n",
    "\n",
    "#function to do multi-headed attention \n",
    "\n",
    "\n",
    "def batch(input):\n",
    "    if input=='train':\n",
    "        data=train_data\n",
    "    else: \n",
    "        data=test_data\n",
    "    index=torch.randint(len(data)-contextual_length, (head_size,)) #grabbing head_size number of random offsets/index is going to be #defined by head_size randomly generated between the length of data and our contextual_length\n",
    "    contextual=torch.stack([data[i:i+contextual_length] for i in index]) #all become a row in a 4 by 8 tensor \n",
    "    prediction=torch.stack([data[i+1:i+contextual_length+1] for i in index])\n",
    "    return contextual, prediction\n",
    "\n",
    "cont, pred=batch('train')\n",
    "print('inputs:')\n",
    "print(cont.shape)\n",
    "print(cont)\n",
    "print('prediction:')\n",
    "print(pred.shape)\n",
    "print(pred)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(head_size): #number of heads\n",
    "    for c in range(contextual_length): \n",
    "        context=cont[b, :c+1]\n",
    "        prediction=pred[b, c]\n",
    "        print(f\"when input is {context.tolist()} the prediction: {prediction}\") \n",
    "\n",
    "\n",
    "#we are getting 4 rows (represent the heads) \n",
    "#we get 8 columns (represent the contextual part of it)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0c6e0b-9651-46e7-af83-8e6f634301c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
